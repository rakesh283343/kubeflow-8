{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titanic_kfp_pipeline.ipynb\n",
    "#Copyright 2020 Google LLC. \n",
    "#This software is provided as-is, without warranty or representation for any use or purpose. \n",
    "#Your use of it is subject to your agreements with Google.\n",
    "#Author: whjang@google.com\n",
    "\n",
    "PROJECT_ID = 'kubeflow-test'\n",
    "IMAGE_PREFIX = 'whjang-titanic'\n",
    "PREPROC_DIR = 'preprocess'\n",
    "TRAIN_DIR = 'train'\n",
    "\n",
    "WORK_BUCKET = 'gs://kubeflow-test-kubeflowpipelines-default'\n",
    "RAW_CSV_GCS_URI = WORK_BUCKET + '/rawdata/train.csv'\n",
    "PREPROC_CSV_GCS_URI = WORK_BUCKET + '/preprocdata/processed_train.csv'\n",
    "ACC_CSV_GCS_URI = WORK_BUCKET + '/latestacc/accuracy.csv'\n",
    "MODEL_PKL_GCS_URI = WORK_BUCKET + '/model/model.pkl'\n",
    "STAGE_GCS_FOLDER = WORK_BUCKET + '/stage'\n",
    "\n",
    "AIPJOB_TRAINER_GCS_PATH = WORK_BUCKET + '/train/titanic_train.tar.gz'\n",
    "AIPJOB_OUTPUT_GCS_PATH = WORK_BUCKET + '/train/output/'\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"IMAGE_PREFIX\"] = IMAGE_PREFIX\n",
    "os.environ[\"PREPROC_DIR\"] = PREPROC_DIR\n",
    "os.environ[\"TRAIN_DIR\"] = TRAIN_DIR\n",
    "os.environ[\"WORK_BUCKET\"] = WORK_BUCKET\n",
    "os.environ[\"RAW_CSV_GCS_URI\"] = RAW_CSV_GCS_URI\n",
    "os.environ[\"PREPROC_CSV_GCS_URI\"] = PREPROC_CSV_GCS_URI\n",
    "os.environ[\"ACC_CSV_GCS_URI\"] = ACC_CSV_GCS_URI\n",
    "os.environ[\"MODEL_PKL_GCS_URI\"] = MODEL_PKL_GCS_URI\n",
    "os.environ[\"STAGE_GCS_FOLDER\"] = STAGE_GCS_FOLDER\n",
    "os.environ[\"AIPJOB_TRAINER_GCS_PATH\"] = AIPJOB_TRAINER_GCS_PATH\n",
    "os.environ[\"AIPJOB_OUTPUT_GCS_PATH\"] = AIPJOB_OUTPUT_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/7 : FROM frolvlad/alpine-python-machinelearning\n",
      " ---> 9e141b2fee53\n",
      "Step 2/7 : RUN pip install --upgrade gcsfs argparse\n",
      " ---> Using cache\n",
      " ---> fcdbc53d4b53\n",
      "Step 3/7 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Using cache\n",
      " ---> ef9e9b665d43\n",
      "Step 4/7 : RUN mkdir -p /titanic/src\n",
      " ---> Using cache\n",
      " ---> 409eecd5404e\n",
      "Step 5/7 : COPY . /titanic/src\n",
      " ---> Using cache\n",
      " ---> d48eb57406bc\n",
      "Step 6/7 : WORKDIR /titanic/src\n",
      " ---> Using cache\n",
      " ---> 5a8dcb43b05d\n",
      "Step 7/7 : ENTRYPOINT [\"python\", \"titanic_preprocess.py\"]\n",
      " ---> Using cache\n",
      " ---> 3c5a2d6c7b8b\n",
      "Successfully built 3c5a2d6c7b8b\n",
      "Successfully tagged whjang-titanic-preprocess:latest\n",
      "The push refers to repository [docker.io/insoopark/whjang-titanic-preprocess]\n",
      "\n",
      "\u001b[1B34e95718: Preparing \n",
      "\u001b[1B2d7b02ac: Preparing \n",
      "\u001b[1B7234986d: Preparing \n",
      "\u001b[1B2d9188d2: Preparing \n",
      "\u001b[1B01e901e5: Preparing \n",
      "\u001b[1B9ef47504: Preparing \n",
      "\u001b[2B9ef47504: Layer already exists \u001b[3A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:32b2645a1a8dac0046467b8c0c9b39385560c68d6c9c259013ec64119f83590e size: 1789\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_PREFIX-$PREPROC_DIR $PREPROC_DIR/.\n",
    "!docker tag $IMAGE_PREFIX-$PREPROC_DIR:latest docker.io/insoopark/$IMAGE_PREFIX-$PREPROC_DIR:latest\n",
    "!docker push docker.io/insoopark/$IMAGE_PREFIX-$PREPROC_DIR:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Titanic Data\n",
      "   Pclass  SibSp  Sex_female  Sex_male  Survived\n",
      "0       3      1           0         1         0\n",
      "1       1      1           1         0         1\n",
      "2       3      0           1         0         1\n",
      "3       1      1           1         0         1\n",
      "4       3      0           0         1         0\n"
     ]
    }
   ],
   "source": [
    "!docker run docker.io/insoopark/$IMAGE_PREFIX-$PREPROC_DIR:latest --raw_csv_gcs_uri $RAW_CSV_GCS_URI --preproc_csv_gcs_uri $PREPROC_CSV_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  18.94kB\n",
      "Step 1/7 : FROM frolvlad/alpine-python-machinelearning\n",
      " ---> 9e141b2fee53\n",
      "Step 2/7 : RUN pip3 install google-cloud-storage==1.0.0 gcsfs sklearn argparse simplejson\n",
      " ---> Running in cd63a60eddb1\n",
      "Collecting google-cloud-storage==1.0.0\n",
      "  Downloading google_cloud_storage-1.0.0-py2.py3-none-any.whl (73 kB)\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-0.6.2-py2.py3-none-any.whl (17 kB)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting simplejson\n",
      "  Downloading simplejson-3.17.2.tar.gz (83 kB)\n",
      "Collecting google-cloud-core<0.25dev,>=0.24.0\n",
      "  Downloading google_cloud_core-0.24.1-py2.py3-none-any.whl (52 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading fsspec-0.8.0-py3-none-any.whl (85 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-1.20.1-py2.py3-none-any.whl (91 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3.8/site-packages (from sklearn) (0.23.1)\n",
      "Collecting googleapis-common-protos>=1.3.4\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting httplib2>=0.9.1\n",
      "  Downloading httplib2-0.18.1-py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3.8/site-packages (from google-cloud-core<0.25dev,>=0.24.0->google-cloud-storage==1.0.0) (1.15.0)\n",
      "Collecting protobuf>=3.0.0\n",
      "  Downloading protobuf-3.13.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting google-auth-httplib2\n",
      "  Downloading google_auth_httplib2-0.0.4-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (47.3.1)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: sklearn, simplejson\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=f529bd56002ec80b9f070a21b992c0184b66f5f9ab27ed4adbb7e87d00df367f\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "  Building wheel for simplejson (setup.py): started\n",
      "  Building wheel for simplejson (setup.py): finished with status 'done'\n",
      "  Created wheel for simplejson: filename=simplejson-3.17.2-py3-none-any.whl size=55459 sha256=5fdc79c12e0b7d801384261c0b6a7057bef436eac516cbf73bb01b391a850493\n",
      "  Stored in directory: /root/.cache/pip/wheels/17/72/7d/df0984c925921e22322ea462a6f861e9d0617881192deb9b8d\n",
      "Successfully built sklearn simplejson\n",
      "Installing collected packages: protobuf, googleapis-common-protos, httplib2, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-httplib2, google-cloud-core, google-cloud-storage, decorator, fsspec, idna, certifi, urllib3, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, gcsfs, sklearn, argparse, simplejson\n",
      "Successfully installed argparse-1.4.0 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 decorator-4.4.2 fsspec-0.8.0 gcsfs-0.6.2 google-auth-1.20.1 google-auth-httplib2-0.0.4 google-auth-oauthlib-0.4.1 google-cloud-core-0.24.1 google-cloud-storage-1.0.0 googleapis-common-protos-1.52.0 httplib2-0.18.1 idna-2.10 oauthlib-3.1.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 simplejson-3.17.2 sklearn-0.0 urllib3-1.25.10\n",
      "\u001b[91mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container cd63a60eddb1\n",
      " ---> 739687dc0b18\n",
      "Step 3/7 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Running in 67661892a7d8\n",
      "Removing intermediate container 67661892a7d8\n",
      " ---> ee40dc91d997\n",
      "Step 4/7 : RUN mkdir -p /titanic/src\n",
      " ---> Running in d5446684f72e\n",
      "Removing intermediate container d5446684f72e\n",
      " ---> f37d4121c957\n",
      "Step 5/7 : COPY . /titanic/src\n",
      " ---> bbe81cf58218\n",
      "Step 6/7 : WORKDIR /titanic/src\n",
      " ---> Running in ab999370fa8a\n",
      "Removing intermediate container ab999370fa8a\n",
      " ---> bee4627620e4\n",
      "Step 7/7 : ENTRYPOINT [\"python\", \"titanic_train.py\"]\n",
      " ---> Running in 41374ac1c013\n",
      "Removing intermediate container 41374ac1c013\n",
      " ---> 4a1652e3f67f\n",
      "Successfully built 4a1652e3f67f\n",
      "Successfully tagged whjang-titanic-train:latest\n",
      "The push refers to repository [docker.io/insoopark/whjang-titanic-train]\n",
      "\n",
      "\u001b[1B4351b6c3: Preparing \n",
      "\u001b[1Bd927906e: Preparing \n",
      "\u001b[1B8c342a17: Preparing \n",
      "\u001b[1B2d9188d2: Preparing \n",
      "\u001b[1B01e901e5: Preparing \n",
      "\u001b[1B9ef47504: Preparing \n",
      "\u001b[5B8c342a17: Pushed   17.73MB/16.28MBA\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2Klatest: digest: sha256:0bec72ea40cafe68e26d492dd2c77278a28222cb3d481bd6a42ada606e7bcc7b size: 1789\n",
      "train titanic model\n",
      "RF Model Score :  0.8078541374474053\n",
      "evaluation model\n",
      "No accuracy file, we will create one\n",
      "confusion matrix \n",
      "[[96 16]\n",
      " [25 41]]\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       112\n",
      "           1       0.72      0.62      0.67        66\n",
      "\n",
      "    accuracy                           0.77       178\n",
      "   macro avg       0.76      0.74      0.75       178\n",
      "weighted avg       0.77      0.77      0.77       178\n",
      "\n",
      "\n",
      "accuracy score\n",
      "0.7696629213483146\n",
      "\n",
      "Writing matcis file: /mlpipeline-metrics.json\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_PREFIX-$TRAIN_DIR $TRAIN_DIR/.\n",
    "!docker tag $IMAGE_PREFIX-$TRAIN_DIR:latest docker.io/insoopark/$IMAGE_PREFIX-$TRAIN_DIR:latest\n",
    "!docker push docker.io/insoopark/$IMAGE_PREFIX-$TRAIN_DIR:latest\n",
    "!docker run docker.io/insoopark/$IMAGE_PREFIX-$TRAIN_DIR:latest \\\n",
    "--preproc_csv_gcs_uri $PREPROC_CSV_GCS_URI \\\n",
    "--model_pkl_gcs_uri $MODEL_PKL_GCS_URI \\\n",
    "--acc_csv_gcs_uri $ACC_CSV_GCS_URI \\\n",
    "--min_acc_progress 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handson.ipynb\n",
      "katib_mnist.py\n",
      "preprocess/\n",
      "preprocess/.ipynb_checkpoints/\n",
      "preprocess/.ipynb_checkpoints/Dockerfile-checkpoint\n",
      "preprocess/.ipynb_checkpoints/titanic_preprocess-checkpoint.py\n",
      "preprocess/titanic_preprocess.py\n",
      "preprocess/Dockerfile\n",
      "runjob.sh\n",
      "setup.py\n",
      "titanic_kfp_pipeline.ipynb\n",
      "train/\n",
      "train/.ipynb_checkpoints/\n",
      "train/.ipynb_checkpoints/titanic_train-checkpoint.py\n",
      "train/.ipynb_checkpoints/Dockerfile-checkpoint\n",
      "train/.ipynb_checkpoints/__init__-checkpoint.py\n",
      "train/titanic_train.py\n",
      "train/__init__.py\n",
      "train/Dockerfile\n",
      "Copying file://titanic_train.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][ 23.5 KiB/ 23.5 KiB]                                                \n",
      "Operation completed over 1 objects/23.5 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!rm -fr titanic_train.tar.gz\n",
    "!tar zcvf titanic_train.tar.gz *\n",
    "!gsutil cp titanic_train.tar.gz $AIPJOB_TRAINER_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U kfp\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.components import func_to_container_op\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "PIPELINE_HOST = \"45b30b46974186c0-dot-us-central2.pipelines.googleusercontent.com\"\n",
    "WORK_BUCKET = \"gs://kubeflow-test-kubeflowpipelines-default\"\n",
    "EXPERIMENT_NAME = \"Titanic Draft Experiment\"\n",
    "\n",
    "# Function for determine deployment\n",
    "@func_to_container_op\n",
    "def check_and_deploy_op(ACC_CSV_GCS_URI) -> str:\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gcsfs\"])\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"ACC_CSV_GCS_URI: \", ACC_CSV_GCS_URI)\n",
    "    \n",
    "    acc_df = pd.read_csv(ACC_CSV_GCS_URI)\n",
    "    return acc_df[\"deploy\"].item()\n",
    "\n",
    "@func_to_container_op\n",
    "def finish_deploy_op(ACC_CSV_GCS_URI):\n",
    " import sys, subprocess\n",
    " subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    " subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gcsfs\"])\n",
    " import pandas as pd\n",
    " acc_df = pd.read_csv(ACC_CSV_GCS_URI)\n",
    " acc_df[\"deploy\"] = \"done\"\n",
    " acc_df.to_csv(ACC_CSV_GCS_URI)\n",
    " print(\"Successfully new model was deployed\")\n",
    "    \n",
    "@dsl.pipeline(\n",
    " name=\"titanic-kubeflow-pipeline-demo\",\n",
    " description = \"Titanic Kubeflow Pipelines demo embrassing AI Platform in Google Cloud\"\n",
    ")\n",
    "\n",
    "def titanic_pipeline(\n",
    " PROJECT_ID,\n",
    " WORK_BUCKET,\n",
    " RAW_CSV_GCS_URI,\n",
    " PREPROC_CSV_GCS_URI,\n",
    " ACC_CSV_GCS_URI,\n",
    " MODEL_PKL_GCS_URI,\n",
    " MIN_ACC_PROGRESS,\n",
    " STAGE_GCS_FOLDER,\n",
    " TRAIN_ON_CLOUD,\n",
    " AIPJOB_TRAINER_GCS_PATH,\n",
    " AIPJOB_OUTPUT_GCS_PATH\n",
    "):\n",
    "    IMAGE_PREFIX = \"whjang-titanic\"\n",
    "    PREPROC_DIR = \"preprocess\"\n",
    "    TRAIN_DIR = \"train\"\n",
    "    MODEL_DIR = \"model\"\n",
    " \n",
    "    preprocess = dsl.ContainerOp(\n",
    "        name = \"Preprocess raw data and generate new one\",\n",
    "        #image = \"gcr.io/\" + str(PROJECT_ID) + \"/\" + IMAGE_PREFIX + \"-\" + PREPROC_DIR + \":latest\",\n",
    "        image = \"docker.io/insoopark/\" + IMAGE_PREFIX + \"-\" + PREPROC_DIR + \":latest\",\n",
    "        arguments = [\n",
    "            \"--raw_csv_gcs_uri\", RAW_CSV_GCS_URI,\n",
    "            \"--preproc_csv_gcs_uri\", PREPROC_CSV_GCS_URI\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_args = [\n",
    "        \"--preproc_csv_gcs_uri\", str(PREPROC_CSV_GCS_URI),\n",
    "        \"--model_pkl_gcs_uri\", str(MODEL_PKL_GCS_URI),\n",
    "        \"--acc_csv_gcs_uri\", str(ACC_CSV_GCS_URI),\n",
    "        \"--min_acc_progress\", str(MIN_ACC_PROGRESS)\n",
    "    ]\n",
    " \n",
    "    with dsl.Condition(TRAIN_ON_CLOUD == False) as check_condition1:\n",
    "        train = dsl.ContainerOp(\n",
    "            name = \"Train\",\n",
    "            image = \"docker.io/insoopark/\" + IMAGE_PREFIX + \"-\" + TRAIN_DIR + \":latest\",\n",
    "            arguments = train_args,\n",
    "                file_outputs={\n",
    "                    \"mlpipeline-metrics\" : \"/mlpipeline-metrics.json\"\n",
    "                }\n",
    "        )\n",
    " \n",
    "    # added by ispark\n",
    "    import json\n",
    "    with dsl.Condition(TRAIN_ON_CLOUD == True) as check_condition2:\n",
    "        aip_job_train_op = comp.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/train/component.yaml\")\n",
    "        help(aip_job_train_op)\n",
    "        aip_train = aip_job_train_op(\n",
    "            project_id=PROJECT_ID, \n",
    "            python_module=\"train.titanic_train\", \n",
    "            package_uris=json.dumps([str(AIPJOB_TRAINER_GCS_PATH)]), \n",
    "            region=\"us-west1\", \n",
    "            args=json.dumps(train_args),\n",
    "            job_dir=AIPJOB_OUTPUT_GCS_PATH, \n",
    "            python_version=\"3.7\",\n",
    "            runtime_version=\"1.15\", #cf. 2.1 \n",
    "            master_image_uri=\"\", \n",
    "            worker_image_uri=\"\", \n",
    "            training_input=\"\", \n",
    "            job_id_prefix=\"\", \n",
    "            job_id=\"\",\n",
    "            wait_interval=5\n",
    "        )\n",
    " \n",
    "    check_deploy = check_and_deploy_op(ACC_CSV_GCS_URI)\n",
    "    with dsl.Condition(check_deploy.output == \"pending\"):\n",
    "        aip_model_deploy_op = comp.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/deploy/component.yaml\")\n",
    "        help(aip_model_deploy_op)\n",
    "        aip_model_deploy = aip_model_deploy_op(\n",
    "            model_uri=str(WORK_BUCKET) + \"/\" + MODEL_DIR, \n",
    "            project_id=PROJECT_ID, \n",
    "            model_id=\"\", \n",
    "            version_id=\"\", \n",
    "            runtime_version=\"1.15\", #cf. 2.1 \n",
    "            python_version=\"3.7\",\n",
    "            version=\"\", \n",
    "            replace_existing_version=\"False\", \n",
    "            set_default=\"True\", \n",
    "            wait_interval=5\n",
    "        )\n",
    "        lastStep = finish_deploy_op(ACC_CSV_GCS_URI)\n",
    " \n",
    "    check_condition1.after(preprocess)\n",
    "    check_condition2.after(preprocess)\n",
    "    check_deploy.after(aip_train)\n",
    "    lastStep.after(aip_model_deploy)\n",
    " \n",
    "    train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    aip_train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    check_deploy.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    aip_model_deploy.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    lastStep.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    " \n",
    "args = {\n",
    "    \"PROJECT_ID\" : \"aiplatformdemo\",\n",
    "    \"WORK_BUCKET\" : WORK_BUCKET,\n",
    "    \"RAW_CSV_GCS_URI\" : WORK_BUCKET + \"/rawdata/train.csv\",\n",
    "    \"PREPROC_CSV_GCS_URI\" : WORK_BUCKET + \"/preprocdata/processed_train.csv\",\n",
    "    \"ACC_CSV_GCS_URI\" : WORK_BUCKET + \"/latestacc/accuracy.csv\",\n",
    "    \"MODEL_PKL_GCS_URI\" : WORK_BUCKET + \"/model/model.pkl\",\n",
    "    \"MIN_ACC_PROGRESS\" : 0.000001,\n",
    "    \"STAGE_GCS_FOLDER\" : WORK_BUCKET + \"/stage\",\n",
    "    \"TRAIN_ON_CLOUD\" : False,\n",
    "    \"AIPJOB_TRAINER_GCS_PATH\" : WORK_BUCKET + \"/train/titanic_train.tar.gz\",\n",
    "    \"AIPJOB_OUTPUT_GCS_PATH\" : WORK_BUCKET + \"/train/output/\"\n",
    "}\n",
    "\n",
    "client = kfp.Client(host=PIPELINE_HOST)\n",
    "#pipeline_name = “titanic_pipelines.zip”\n",
    "#compiler.Compiler().compile(titanic_pipeline, pipeline_name)\n",
    "#try:\n",
    "# pipeline = client.upload_pipeline(pipeline_package_path=pipeline_name, pipeline_name=pipeline_name)\n",
    "# print(“uploaded:” + pipeline.id)\n",
    "#except:\n",
    "# print(“already exist”)\n",
    "client.create_run_from_pipeline_func(\n",
    "    titanic_pipeline,\n",
    "    arguments=args,\n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
