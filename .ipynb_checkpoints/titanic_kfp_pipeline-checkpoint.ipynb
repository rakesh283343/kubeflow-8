{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titanic_kfp_pipeline.ipynb\n",
    "#Copyright 2020 Google LLC. \n",
    "#This software is provided as-is, without warranty or representation for any use or purpose. \n",
    "#Your use of it is subject to your agreements with Google.\n",
    "#Author: whjang@google.com\n",
    "\n",
    "PROJECT_ID = 'kubeflow-test'\n",
    "IMAGE_PREFIX = 'whjang-titanic'\n",
    "PREPROC_DIR = 'preprocess'\n",
    "TRAIN_DIR = 'train'\n",
    "\n",
    "WORK_BUCKET = 'gs://kubeflow-test-kubeflowpipelines-default'\n",
    "RAW_CSV_GCS_URI = WORK_BUCKET + '/rawdata/train.csv'\n",
    "PREPROC_CSV_GCS_URI = WORK_BUCKET + '/preprocdata/processed_train.csv'\n",
    "ACC_CSV_GCS_URI = WORK_BUCKET + '/latestacc/accuracy.csv'\n",
    "MODEL_PKL_GCS_URI = WORK_BUCKET + '/model/model.pkl'\n",
    "STAGE_GCS_FOLDER = WORK_BUCKET + '/stage'\n",
    "\n",
    "AIPJOB_TRAINER_GCS_PATH = WORK_BUCKET + '/train/titanic_train.tar.gz'\n",
    "AIPJOB_OUTPUT_GCS_PATH = WORK_BUCKET + '/train/output/'\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"IMAGE_PREFIX\"] = IMAGE_PREFIX\n",
    "os.environ[\"PREPROC_DIR\"] = PREPROC_DIR\n",
    "os.environ[\"TRAIN_DIR\"] = TRAIN_DIR\n",
    "os.environ[\"WORK_BUCKET\"] = WORK_BUCKET\n",
    "os.environ[\"RAW_CSV_GCS_URI\"] = RAW_CSV_GCS_URI\n",
    "os.environ[\"PREPROC_CSV_GCS_URI\"] = PREPROC_CSV_GCS_URI\n",
    "os.environ[\"ACC_CSV_GCS_URI\"] = ACC_CSV_GCS_URI\n",
    "os.environ[\"MODEL_PKL_GCS_URI\"] = MODEL_PKL_GCS_URI\n",
    "os.environ[\"STAGE_GCS_FOLDER\"] = STAGE_GCS_FOLDER\n",
    "os.environ[\"AIPJOB_TRAINER_GCS_PATH\"] = AIPJOB_TRAINER_GCS_PATH\n",
    "os.environ[\"AIPJOB_OUTPUT_GCS_PATH\"] = AIPJOB_OUTPUT_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  8.704kB\n",
      "Step 1/7 : FROM frolvlad/alpine-python-machinelearning\n",
      " ---> 9e141b2fee53\n",
      "Step 2/7 : RUN pip install --upgrade gcsfs argparse\n",
      " ---> Using cache\n",
      " ---> fcdbc53d4b53\n",
      "Step 3/7 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Using cache\n",
      " ---> ef9e9b665d43\n",
      "Step 4/7 : RUN mkdir -p /titanic/src\n",
      " ---> Using cache\n",
      " ---> 409eecd5404e\n",
      "Step 5/7 : COPY . /titanic/src\n",
      " ---> Using cache\n",
      " ---> d48eb57406bc\n",
      "Step 6/7 : WORKDIR /titanic/src\n",
      " ---> Using cache\n",
      " ---> 5a8dcb43b05d\n",
      "Step 7/7 : ENTRYPOINT [\"python\", \"titanic_preprocess.py\"]\n",
      " ---> Using cache\n",
      " ---> 3c5a2d6c7b8b\n",
      "Successfully built 3c5a2d6c7b8b\n",
      "Successfully tagged whjang-titanic-preprocess:latest\n",
      "The push refers to repository [docker.io/insoopark/whjang-titanic-preprocess]\n",
      "\n",
      "\u001b[1B34e95718: Preparing \n",
      "\u001b[1B2d7b02ac: Preparing \n",
      "\u001b[1B7234986d: Preparing \n",
      "\u001b[1B2d9188d2: Preparing \n",
      "\u001b[1B01e901e5: Preparing \n",
      "\u001b[1B9ef47504: Preparing \n",
      "\u001b[2B9ef47504: Layer already exists \u001b[3A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:32b2645a1a8dac0046467b8c0c9b39385560c68d6c9c259013ec64119f83590e size: 1789\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_PREFIX-$PREPROC_DIR $PREPROC_DIR/.\n",
    "!docker tag $IMAGE_PREFIX-$PREPROC_DIR:latest docker.io/insoopark/$IMAGE_PREFIX-$PREPROC_DIR:latest\n",
    "!docker push docker.io/insoopark/$IMAGE_PREFIX-$PREPROC_DIR:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Titanic Data\n",
      "   Pclass  SibSp  Sex_female  Sex_male  Survived\n",
      "0       3      1           0         1         0\n",
      "1       1      1           1         0         1\n",
      "2       3      0           1         0         1\n",
      "3       1      1           1         0         1\n",
      "4       3      0           0         1         0\n"
     ]
    }
   ],
   "source": [
    "!docker run docker.io/insoopark/$IMAGE_PREFIX-$PREPROC_DIR:latest --raw_csv_gcs_uri $RAW_CSV_GCS_URI --preproc_csv_gcs_uri $PREPROC_CSV_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  18.94kB\n",
      "Step 1/7 : FROM frolvlad/alpine-python-machinelearning\n",
      " ---> 9e141b2fee53\n",
      "Step 2/7 : RUN pip3 install google-cloud-storage==1.0.0 gcsfs sklearn argparse simplejson\n",
      " ---> Using cache\n",
      " ---> 739687dc0b18\n",
      "Step 3/7 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Using cache\n",
      " ---> ee40dc91d997\n",
      "Step 4/7 : RUN mkdir -p /titanic/src\n",
      " ---> Using cache\n",
      " ---> f37d4121c957\n",
      "Step 5/7 : COPY . /titanic/src\n",
      " ---> 7839a6e2f78c\n",
      "Step 6/7 : WORKDIR /titanic/src\n",
      " ---> Running in 235dd6044f7e\n",
      "Removing intermediate container 235dd6044f7e\n",
      " ---> e364df3124b9\n",
      "Step 7/7 : ENTRYPOINT [\"python\", \"titanic_train.py\"]\n",
      " ---> Running in fb74ac0e468f\n",
      "Removing intermediate container fb74ac0e468f\n",
      " ---> be3c7ffadd1c\n",
      "Successfully built be3c7ffadd1c\n",
      "Successfully tagged whjang-titanic-train:latest\n",
      "The push refers to repository [docker.io/insoopark/whjang-titanic-train]\n",
      "\n",
      "\u001b[1Ba6efe7eb: Preparing \n",
      "\u001b[1Bd927906e: Preparing \n",
      "\u001b[1B8c342a17: Preparing \n",
      "\u001b[1B2d9188d2: Preparing \n",
      "\u001b[1B01e901e5: Preparing \n",
      "\u001b[1B9ef47504: Preparing \n",
      "\u001b[7Ba6efe7eb: Pushed lready exists 6kBA\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:12dfd1a5413e90a62523d971f87061457c0002abaf7621648bb1b27680480c75 size: 1789\n",
      "train titanic model\n",
      "RF Model Score :  0.814866760168303\n",
      "evaluation model\n",
      "No accuracy file, we will create one\n",
      "confusion matrix \n",
      "[[89 15]\n",
      " [28 46]]\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81       104\n",
      "           1       0.75      0.62      0.68        74\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.76      0.74      0.74       178\n",
      "weighted avg       0.76      0.76      0.75       178\n",
      "\n",
      "\n",
      "accuracy score\n",
      "0.7584269662921348\n",
      "\n",
      "Writing matcis file: /mlpipeline-metrics.json\n",
      "latestacc: 0.0\n",
      "min_acc_progress: 1e-06\n",
      "File model.pkl uploaded\n",
      "Write to GCS:gs://kubeflow-test-kubeflowpipelines-default/latestacc/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_PREFIX-$TRAIN_DIR $TRAIN_DIR/.\n",
    "!docker tag $IMAGE_PREFIX-$TRAIN_DIR:latest docker.io/insoopark/$IMAGE_PREFIX-$TRAIN_DIR:latest\n",
    "!docker push docker.io/insoopark/$IMAGE_PREFIX-$TRAIN_DIR:latest\n",
    "!docker run docker.io/insoopark/$IMAGE_PREFIX-$TRAIN_DIR:latest \\\n",
    "--preproc_csv_gcs_uri $PREPROC_CSV_GCS_URI \\\n",
    "--model_pkl_gcs_uri $MODEL_PKL_GCS_URI \\\n",
    "--acc_csv_gcs_uri $ACC_CSV_GCS_URI \\\n",
    "--min_acc_progress 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handson.ipynb\n",
      "katib_mnist.py\n",
      "preprocess/\n",
      "preprocess/.ipynb_checkpoints/\n",
      "preprocess/.ipynb_checkpoints/Dockerfile-checkpoint\n",
      "preprocess/.ipynb_checkpoints/titanic_preprocess-checkpoint.py\n",
      "preprocess/titanic_preprocess.py\n",
      "preprocess/Dockerfile\n",
      "runjob.sh\n",
      "setup.py\n",
      "titanic_kfp_pipeline.ipynb\n",
      "train/\n",
      "train/.ipynb_checkpoints/\n",
      "train/.ipynb_checkpoints/titanic_train-checkpoint.py\n",
      "train/.ipynb_checkpoints/Dockerfile-checkpoint\n",
      "train/.ipynb_checkpoints/__init__-checkpoint.py\n",
      "train/titanic_train.py\n",
      "train/__init__.py\n",
      "train/Dockerfile\n",
      "Copying file://titanic_train.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][ 19.6 KiB/ 19.6 KiB]                                                \n",
      "Operation completed over 1 objects/19.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!rm -fr titanic_train.tar.gz\n",
    "!tar zcvf titanic_train.tar.gz *\n",
    "!gsutil cp titanic_train.tar.gz $AIPJOB_TRAINER_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kfp in /opt/conda/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: kfp-server-api<2.0.0,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: Deprecated in /opt/conda/lib/python3.7/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: strip-hints in /opt/conda/lib/python3.7/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<12.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (11.0.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from requests-toolbelt>=0.8.0->kfp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (49.2.0.post20200712)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=0.8.0->kfp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=0.8.0->kfp) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Help on function Submitting a Cloud ML training job as a pipeline step:\n",
      "\n",
      "Submitting a Cloud ML training job as a pipeline step(project_id: 'GCPProjectID', python_module: str = '', package_uris: list = '', region: 'GCPRegion' = '', args: list = '', job_dir: 'GCSPath' = '', python_version: str = '', runtime_version: str = '', master_image_uri: 'GCRPath' = '', worker_image_uri: 'GCRPath' = '', training_input: dict = '', job_id_prefix: str = '', job_id: str = '', wait_interval: int = '30')\n",
      "    Submitting a Cloud ML training job as a pipeline step\n",
      "    A Kubeflow Pipeline component to submit a Cloud Machine Learning (Cloud ML) \n",
      "    Engine training job as a step in a pipeline.\n",
      "\n",
      "Help on function Deploying a trained model to Cloud Machine Learning Engine:\n",
      "\n",
      "Deploying a trained model to Cloud Machine Learning Engine(model_uri: 'GCSPath', project_id: 'GCPProjectID', model_id: str = '', version_id: str = '', runtime_version: str = '', python_version: str = '', model: dict = '', version: dict = '', replace_existing_version: 'Bool' = 'Fasle', set_default: 'Bool' = 'False', wait_interval: int = '30')\n",
      "    Deploying a trained model to Cloud Machine Learning Engine\n",
      "    A Kubeflow Pipeline component to deploy a trained model from a Cloud Storage\n",
      "    path to a Cloud Machine Learning Engine service.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Deploying a trained model to Cloud Machine Learning Engine() got an unexpected keyword argument 'sa_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f7b521a4cc85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mtitanic_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEXPERIMENT_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mcreate_run_from_pipeline_func\u001b[0;34m(self, pipeline_func, arguments, run_name, experiment_name, pipeline_conf, namespace)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0mpipeline_package_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m       \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m    903\u001b[0m           \u001b[0mpipeline_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m           \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m           package_path=package_path)\n\u001b[0m\u001b[1;32m    906\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check_old_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mpipeline_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         pipeline_conf)\n\u001b[0m\u001b[1;32m    961\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0m_validate_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m       \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mpipeline_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_conf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;31m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-f7b521a4cc85>\u001b[0m in \u001b[0;36mtitanic_pipeline\u001b[0;34m(PROJECT_ID, WORK_BUCKET, RAW_CSV_GCS_URI, PREPROC_CSV_GCS_URI, ACC_CSV_GCS_URI, MODEL_PKL_GCS_URI, MIN_ACC_PROGRESS, STAGE_GCS_FOLDER, TRAIN_ON_CLOUD, AIPJOB_TRAINER_GCS_PATH, AIPJOB_OUTPUT_GCS_PATH)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mreplace_existing_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"False\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mset_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"True\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mwait_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m         \u001b[0mlastStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinish_deploy_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACC_CSV_GCS_URI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Deploying a trained model to Cloud Machine Learning Engine() got an unexpected keyword argument 'sa_name'"
     ]
    }
   ],
   "source": [
    "!pip3 install -U kfp --upgrade --user\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.components import func_to_container_op\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "PIPELINE_HOST = \"45b30b46974186c0-dot-us-central2.pipelines.googleusercontent.com\"\n",
    "WORK_BUCKET = \"gs://kubeflow-test-kubeflowpipelines-default\"\n",
    "EXPERIMENT_NAME = \"Titanic Draft Experiment\"\n",
    "\n",
    "# Function for determine deployment\n",
    "@func_to_container_op\n",
    "def check_and_deploy_op(ACC_CSV_GCS_URI) -> str:\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gcsfs\"])\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"ACC_CSV_GCS_URI: \", ACC_CSV_GCS_URI)\n",
    "    \n",
    "    acc_df = pd.read_csv(ACC_CSV_GCS_URI)\n",
    "    return acc_df[\"deploy\"].item()\n",
    "\n",
    "@func_to_container_op\n",
    "def finish_deploy_op(ACC_CSV_GCS_URI):\n",
    " import sys, subprocess\n",
    " subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    " subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gcsfs\"])\n",
    " import pandas as pd\n",
    " acc_df = pd.read_csv(ACC_CSV_GCS_URI)\n",
    " acc_df[\"deploy\"] = \"done\"\n",
    " acc_df.to_csv(ACC_CSV_GCS_URI)\n",
    " print(\"Successfully new model was deployed\")\n",
    "    \n",
    "@dsl.pipeline(\n",
    " name=\"titanic-kubeflow-pipeline-demo\",\n",
    " description = \"Titanic Kubeflow Pipelines demo embrassing AI Platform in Google Cloud\"\n",
    ")\n",
    "\n",
    "def titanic_pipeline(\n",
    "    PROJECT_ID,\n",
    "    WORK_BUCKET,\n",
    "    RAW_CSV_GCS_URI,\n",
    "    PREPROC_CSV_GCS_URI,\n",
    "    ACC_CSV_GCS_URI,\n",
    "    MODEL_PKL_GCS_URI,\n",
    "    MIN_ACC_PROGRESS,\n",
    "    STAGE_GCS_FOLDER,\n",
    "    TRAIN_ON_CLOUD,\n",
    "    AIPJOB_TRAINER_GCS_PATH,\n",
    "    AIPJOB_OUTPUT_GCS_PATH\n",
    "):\n",
    "    IMAGE_PREFIX = \"whjang-titanic\"\n",
    "    PREPROC_DIR = \"preprocess\"\n",
    "    TRAIN_DIR = \"train\"\n",
    "    MODEL_DIR = \"model\"\n",
    " \n",
    "    preprocess = dsl.ContainerOp(\n",
    "        name = \"Preprocess raw data and generate new one\",\n",
    "        #image = \"gcr.io/\" + str(PROJECT_ID) + \"/\" + IMAGE_PREFIX + \"-\" + PREPROC_DIR + \":latest\",\n",
    "        image = \"docker.io/insoopark/\" + IMAGE_PREFIX + \"-\" + PREPROC_DIR + \":latest\",\n",
    "        arguments = [\n",
    "            \"--raw_csv_gcs_uri\", RAW_CSV_GCS_URI,\n",
    "            \"--preproc_csv_gcs_uri\", PREPROC_CSV_GCS_URI\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_args = [\n",
    "        \"--preproc_csv_gcs_uri\", str(PREPROC_CSV_GCS_URI),\n",
    "        \"--model_pkl_gcs_uri\", str(MODEL_PKL_GCS_URI),\n",
    "        \"--acc_csv_gcs_uri\", str(ACC_CSV_GCS_URI),\n",
    "        \"--min_acc_progress\", str(MIN_ACC_PROGRESS)\n",
    "    ]\n",
    " \n",
    "    with dsl.Condition(TRAIN_ON_CLOUD == False) as check_condition1:\n",
    "        train = dsl.ContainerOp(\n",
    "            name = \"Train\",\n",
    "            image = \"docker.io/insoopark/\" + IMAGE_PREFIX + \"-\" + TRAIN_DIR + \":latest\",\n",
    "            arguments = train_args,\n",
    "                file_outputs={\n",
    "                    \"mlpipeline-metrics\" : \"/mlpipeline-metrics.json\"\n",
    "                }\n",
    "        )\n",
    " \n",
    "    # added by ispark\n",
    "    import json\n",
    "    with dsl.Condition(TRAIN_ON_CLOUD == False) as check_condition2:\n",
    "        aip_job_train_op = comp.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/train/component.yaml\")\n",
    "        help(aip_job_train_op)\n",
    "        aip_train = aip_job_train_op(\n",
    "            project_id=PROJECT_ID, \n",
    "            python_module=\"train.titanic_train\", \n",
    "            package_uris=json.dumps([str(AIPJOB_TRAINER_GCS_PATH)]), \n",
    "            region=\"us-west1\", \n",
    "            args=json.dumps(train_args),\n",
    "            job_dir=AIPJOB_OUTPUT_GCS_PATH, \n",
    "            python_version=\"3.7\",\n",
    "            runtime_version=\"1.15\", #cf. 2.1 \n",
    "            master_image_uri=\"\", \n",
    "            worker_image_uri=\"\", \n",
    "            training_input=\"\", \n",
    "            job_id_prefix=\"\", \n",
    "            job_id=\"\",\n",
    "            wait_interval=5\n",
    "        )\n",
    " \n",
    "    check_deploy = check_and_deploy_op(ACC_CSV_GCS_URI)\n",
    "    with dsl.Condition(check_deploy.output == \"pending\"):\n",
    "        aip_model_deploy_op = comp.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/deploy/component.yaml\")\n",
    "        help(aip_model_deploy_op)\n",
    "        aip_model_deploy = aip_model_deploy_op(\n",
    "            model_uri=str(WORK_BUCKET) + \"/\" + MODEL_DIR, \n",
    "            project_id=PROJECT_ID, \n",
    "            model_id=\"\", \n",
    "            version_id=\"\", \n",
    "            runtime_version=\"1.15\", #cf. 2.1 \n",
    "            python_version=\"3.7\",\n",
    "            version=\"\", \n",
    "            replace_existing_version=\"False\", \n",
    "            set_default=\"True\", \n",
    "            wait_interval=5\n",
    "        )\n",
    "        lastStep = finish_deploy_op(ACC_CSV_GCS_URI)\n",
    " \n",
    "    check_condition1.after(preprocess)\n",
    "    check_condition2.after(preprocess)\n",
    "    check_deploy.after(aip_train)\n",
    "    lastStep.after(aip_model_deploy)\n",
    " \n",
    "    train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    aip_train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    check_deploy.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    aip_model_deploy.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    lastStep.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    " \n",
    "args = {\n",
    "    \"PROJECT_ID\" : \"kubeflow-test\",\n",
    "    \"WORK_BUCKET\" : WORK_BUCKET,\n",
    "    \"RAW_CSV_GCS_URI\" : WORK_BUCKET + \"/rawdata/train.csv\",\n",
    "    \"PREPROC_CSV_GCS_URI\" : WORK_BUCKET + \"/preprocdata/processed_train.csv\",\n",
    "    \"ACC_CSV_GCS_URI\" : WORK_BUCKET + \"/latestacc/accuracy.csv\",\n",
    "    \"MODEL_PKL_GCS_URI\" : WORK_BUCKET + \"/model/model.pkl\",\n",
    "    \"MIN_ACC_PROGRESS\" : 0.000001,\n",
    "    \"STAGE_GCS_FOLDER\" : WORK_BUCKET + \"/stage\",\n",
    "    \"TRAIN_ON_CLOUD\" : True,\n",
    "    \"AIPJOB_TRAINER_GCS_PATH\" : WORK_BUCKET + \"/train/titanic_train.tar.gz\",\n",
    "    \"AIPJOB_OUTPUT_GCS_PATH\" : WORK_BUCKET + \"/train/output/\",\n",
    "}\n",
    "\n",
    "client = kfp.Client(host=PIPELINE_HOST, other_client_id=\"insoo67.park\")\n",
    "#pipeline_name = “titanic_pipelines.zip”\n",
    "#compiler.Compiler().compile(titanic_pipeline, pipeline_name)\n",
    "#try:\n",
    "# pipeline = client.upload_pipeline(pipeline_package_path=pipeline_name, pipeline_name=pipeline_name)\n",
    "# print(“uploaded:” + pipeline.id)\n",
    "#except:\n",
    "# print(“already exist”)\n",
    "client.create_run_from_pipeline_func(\n",
    "    titanic_pipeline,\n",
    "    arguments=args,\n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
